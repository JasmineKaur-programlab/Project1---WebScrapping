{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project1(WebScrapping Jobsite).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPOZsCQoKO9bPQimynrs/DB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JasmineKaur-programlab/Project1---WebScrapping/blob/main/Project1(WebScrapping_Jobsite).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBkC62x8CcXv"
      },
      "source": [
        "  import requests\n",
        "\n",
        "  from bs4 import BeautifulSoup\n",
        "\n",
        "  import pandas as pd #need to create dataframe"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B0PO6l-D5vL"
      },
      "source": [
        "First we will extract the page from the function as we perform extract , transform and load\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01nzDTDED29n"
      },
      "source": [
        "   #first is the extract the page\n",
        "   def extract(page):\n",
        "     headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36'}\n",
        "     url = f'https://in.indeed.com/jobs?q=data%20scientist&l=Pune&start={page}&vjk=522b5f4c975a1e2e'\n",
        "     r = requests.get(url,headers) #return r.status_code\n",
        "     #print(extract(0))\n",
        "     soup=BeautifulSoup(r.content,'html.parser')\n",
        "     return soup\n",
        "\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zbJu9OYZfys"
      },
      "source": [
        "We will save this as a function and send it to transform "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2mdKlH8F_Zm"
      },
      "source": [
        "   def transform(soup):\n",
        "     divs = soup.find_all('div')"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dyojDfPD5C-"
      },
      "source": [
        "In the transform we would find the title, comapny, salary ,jobdescription and later use this to find all of them "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g91Xm2pDrFZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5873b7a-e0b4-4ef4-d5a2-85714ecbe41f"
      },
      "source": [
        "    #print(title) \n",
        "     #print(company) \n",
        "     #return\n",
        "     #now created a dictionary to store all the items and to manipulate the list\n",
        "    \n",
        "    def transform(soup):\n",
        "      divs = soup.find_all('div',class_='job_seen_beacon')\n",
        "      for item in divs:\n",
        "        title = item.find('a').text.strip()\n",
        "        company = item.find('span', class_ = 'companyName').text.strip()  #strip used to remove all whitespaces\n",
        "        try:\n",
        "          salary = item.find('span',class_= 'salary-snippet').text.strip() #salary not disclosed everywhere so used try/except\n",
        "        except:\n",
        "          salary =''                  \n",
        "        Jobdescription = item.find('div',{'class':'job-snippet'}).text.strip().replace('\\n','') \n",
        "        #print(title) \n",
        "        #print(company) \n",
        "        #return\n",
        "        #now created a dictionary to store all the items and to manipulate the list\n",
        "        job = {\n",
        "        'title' : title,\n",
        "        'company': company,\n",
        "        'salary' : salary,\n",
        "        'Jobdescription': Jobdescription\n",
        "        }\n",
        "        joblist.append(job)\n",
        "      return  \n",
        "     \n",
        "\n",
        "joblist = []\n",
        "for i in range(0,100,10):\n",
        "  print(f'Getting page,{i}')\n",
        "  c= extract(i)\n",
        "  #print(transform(c))\n",
        "  transform(c)\n",
        "print(len(joblist))\n",
        "  #print(joblist)\n",
        "\n",
        "df = pd.DataFrame(joblist)\n",
        "print(df.head())\n",
        "df.to_csv('jobsscience.csv')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting page,0\n",
            "Getting page,10\n",
            "Getting page,20\n",
            "Getting page,30\n",
            "Getting page,40\n",
            "Getting page,50\n",
            "Getting page,60\n",
            "Getting page,70\n",
            "Getting page,80\n",
            "Getting page,90\n",
            "150\n",
            "                    title  ...                                     Jobdescription\n",
            "0        3RI TECHNOLOGIES  ...  You will lead a range of data analytics effort...\n",
            "1                    Citi  ...  Mines and analyzes data from various banking p...\n",
            "2  KPIT Technologies GmbH  ...  Build methodologies and standards to prepare, ...\n",
            "3                  Maersk  ...  2 Participate in data science products, be han...\n",
            "4                      bp  ...  Compared to data analysts, data scientists hav...\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4ew55BmuD28"
      },
      "source": [
        "So we have the first page of the indeed.com and now we will move to second one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbQy5j0qy27r"
      },
      "source": [
        "As one can see one can scrape the jobportal ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4a6zwpezKAO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}